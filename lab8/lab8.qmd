---
title: "Lab 8"
subtitle: "Anagha Pashilkar"
format: pdf
editor: visual
---

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)

head(wisc.df)

```

```{r}
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
```

```{r}
# Create diagnosis vector for later 
diagnosis <- wisc.df$diagnosis
```

> Q1. How many observations are in this dataset?

```{r}
nrow(wisc.df)
```

Q2. How many of the observations have a malignant diagnosis?

```{r}
sum(diagnosis == "M")
```

Q3. How many variables/features in the data are suffixed with \_mean?

```{r}

length(grep("_mean", colnames(wisc.df)))
```

## Principle Component Analysis

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp( wisc.data, scale=T)
```

```{r}
# Look at summary of results
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

```{r}
# Proportion of variance explained by PC1
prop_var_PC1 <- wisc.pr$sdev[1]^2 / sum(wisc.pr$sdev^2)
prop_var_PC1
```

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

```{r}
# Cumulative proportion of variance
cum_var <- cumsum(wisc.pr$sdev^2) / sum(wisc.pr$sdev^2)

# Number of PCs needed to capture at least 70% of the variance
n_PC_70 <- min(which(cum_var >= 0.7))
n_PC_70
```

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

```{r}
# Number of PCs needed to capture at least 90% of the variance
n_PC_90 <- min(which(cum_var >= 0.9))
n_PC_90
```

### Interpreting PCA Results

```{r}
biplot(wisc.pr)
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why? very difficult to understand. too much going on.

```{r}
# Scatter plot observations by components 1 and 2

plot(wisc.pr$x[, c(1, 2)], col = ifelse(diagnosis == "M", "red", "black"), 
     xlab = "PC1", ylab = "PC2", main = "Scatter plot of PC1 vs. PC2")
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[, c(1, 3)], col = ifelse(diagnosis == "M", "red", "black"), 
     xlab = "PC1", ylab = "PC3", main = "Scatter plot of PC1 vs. PC3")
```

### create ggplot

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point() +
  xlab("PC1") +
  ylab("PC2") +
  ggtitle("Scatter plot of PC1 vs. PC2")

```

### Variance

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

### Communicating PCA Results

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr\$rotation\[,1\]) for the feature concave.points_mean?

```{r}
# Find the component of the loading vector for concave.points_mean for PC1
loading_concave_points_mean <- wisc.pr$rotation["concave.points_mean", 1]
loading_concave_points_mean
```

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
# Calculate cumulative proportion of variance explained by each PC
cumulative_var <- cumsum(pve)

# Find the minimum number of PCs required to explain 80% of variance
min_num_pcs <- min(which(cumulative_var >= 0.8))

min_num_pcs
```

## Hierarchical Clustering

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to `data.dist`.

```{r}
data.dist <- dist(data.scaled)
```

Create a hierarchical clustering model using complete linkage. Manually specify the method argument to `hclust()` and assign the results to `wisc.hclust`.

```{r}
# Create a hierarchical clustering model using complete linkage
wisc.hclust <- hclust(data.dist, method = "complete")
```

### Results of hierarchical clustering

\> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust, main = "Dendrogram of hierarchical clustering model")
abline(h = 19, col = "red", lty = 2)
```

### Selecting number of clusters

```{r}
# Use cutree() to cut the tree so that it has 4 clusters
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
```

We can use the `table()` function to compare the cluster membership to the actual diagnoses.

```{r}
table(wisc.hclust.clusters, diagnosis)
```

> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

```{r}
# Cut the tree into 2-10 clusters and display the resulting cluster vs diagnosis tables
for (k in 2:10) {
  wisc.hclust.clusters <- cutree(wisc.hclust, k)
  tab <- table(wisc.hclust.clusters, diagnosis)
  print(tab)
}
```

> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

ward.D2 since it minimizes variance within clusters. Since there is a lot of data, this method is most useful and will give cleanest results.

### K-means

```{r}
# Create k-means model
wisc.km <- kmeans(scale(wisc.data), centers=2, nstart=20)

# Compare to actual diagnoses
table(wisc.km$cluster, diagnosis)

# Compare to hierarchical clustering
table(wisc.km$cluster, wisc.hclust.clusters)

```

## Combining methods

```{r}
grps <- cutree(wisc.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col= grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=  ifelse(diagnosis == "M", "red", "black"))
```

```{r}
g <- as.factor(grps)
levels(g)

```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```

```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k = 2)
```

> Q15. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```

> It clearly separates the clusters into two diagnoses (M and B)

> Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the `table()` function to compare the output of each model (`wisc.km$cluster` and `wisc.hclust.clusters`) with the vector containing the actual diagnoses.

```{r}
table(wisc.km$cluster, diagnosis)
table(wisc.hclust.clusters, diagnosis)
```

K-means is much more efficient at separating the diagnosis. However, hclust helps us see a relationship between different datapoints at different levels.

## Sensitivity/ Specificity

\> Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?
